{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOuQrU+/2JIZ6Jxseq0egWD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"FJordedy4lg3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23gwq4K_iyhz","executionInfo":{"status":"ok","timestamp":1771247522987,"user_tz":-60,"elapsed":34896,"user":{"displayName":"MARCO PARRELLA","userId":"15188766073724744137"}},"outputId":"1ae7fc3e-6ab6-448b-99a4-1c7f377ec53b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Codice lcs_cuda.cu aggiornato con supporto per matrici grandi (size_t).\n","\n","Compilazione in corso...\n","nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n","Compilazione completata.\n","\n","impl,size,rep,param,time_s\n","cuda,10240,1,128,0.109924\n","cuda,10240,1,256,0.064252\n","cuda,10240,1,512,0.076628\n","cuda,10240,2,128,0.060675\n","cuda,10240,2,256,0.064462\n","cuda,10240,2,512,0.076243\n","cuda,10240,3,128,0.065312\n","cuda,10240,3,256,0.064488\n","cuda,10240,3,512,0.076185\n","cuda,51200,1,128,2.992495\n","cuda,51200,1,256,3.016323\n","cuda,51200,1,512,3.072156\n","cuda,51200,2,128,2.995034\n","cuda,51200,2,256,3.016395\n","cuda,51200,2,512,3.078468\n","cuda,51200,3,128,3.002448\n","cuda,51200,3,256,3.023699\n","cuda,51200,3,512,3.081815\n"]}],"source":["# Assignment: Longest Common Subsequence (HPC Project)\n","#**Student**: Parrella Marco, Matricola: 0622702536, Email: m.parrella21@studenti.unisa.it\n","#**Lecturer**: Moscato Francesco, fmoscato@unisa.it\n","\n","#**License**: GPLv3 (see LICENSE file)\n","#**Requirements**: Implement Parallel LCS (OpenMP, MPI, CUDA)\n","#**Purpose**: Notebook per la compilazione e l'esecuzione dei benchmark della versione CUDA su ambiente con GPU (Google Colab).\n","\n","import os\n","import subprocess\n","\n","# --- PASSO 0: FIX DEL CODICE CUDA (Integer Overflow) ---\n","# Riscriviamo il file .cu aggiungendo i cast a (size_t) per gestire matrici > 46000x46000\n","cuda_code_fixed = r\"\"\"\n","/*\n"," * GPLv3\n"," * CUDA LCS - Anti-diagonal implementation\n"," * UPDATED: Dynamic block size & Correct Timing & LARGE MATRIX FIX (size_t)\n"," */\n","\n","#include <stdio.h>\n","#include <stdlib.h>\n","#include <string.h>\n","#include <cuda.h>\n","#include <cuda_runtime.h>\n","\n","static inline void cuda_check(cudaError_t e, const char *msg) {\n","    if (e != cudaSuccess) { fprintf(stderr, \"%s: %s\\n\", msg, cudaGetErrorString(e)); exit(1); }\n","}\n","\n","__global__ void diag_kernel(int *dmat, const char *A, const char *B, int n, int m, int k) {\n","    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n","\n","    int i_start = max(1, k - m);\n","    int i_end = min(n, k - 1);\n","    int len = i_end - i_start + 1;\n","\n","    if (idx >= len) return;\n","\n","    int i = i_start + idx;\n","    int j = k - i;\n","\n","    // FIX: Usiamo size_t per la larghezza e per il calcolo dell'indice\n","    // Altrimenti 51200*51200 supera il limite dei 32 bit (2 miliardi)\n","    size_t mcols = (size_t)m + 1;\n","\n","    // Calcolo indici con size_t per evitare overflow\n","    size_t idx_cur    = (size_t)i * mcols + j;\n","    size_t idx_up     = (size_t)(i-1) * mcols + j;\n","    size_t idx_left   = (size_t)i * mcols + (j-1);\n","    size_t idx_upleft = (size_t)(i-1) * mcols + (j-1);\n","\n","    if (A[i-1] == B[j-1]) {\n","        dmat[idx_cur] = dmat[idx_upleft] + 1;\n","    } else {\n","        int up = dmat[idx_up];\n","        int left = dmat[idx_left];\n","        dmat[idx_cur] = (up > left ? up : left);\n","    }\n","}\n","\n","int main(int argc, char **argv) {\n","    if (argc < 3) { return 1; }\n","\n","    const char *fileA = argv[1];\n","    const char *fileB = argv[2];\n","    int blockSize = 256;\n","\n","    if (argc > 3) blockSize = atoi(argv[3]);\n","    if (blockSize <= 0) blockSize = 256;\n","\n","    FILE *fa = fopen(fileA, \"rb\");\n","    FILE *fb = fopen(fileB, \"rb\");\n","    if (!fa || !fb) { perror(\"fopen\"); return 1; }\n","\n","    fseek(fa, 0, SEEK_END); int n = ftell(fa); fseek(fa, 0, SEEK_SET);\n","    fseek(fb, 0, SEEK_END); int m = ftell(fb); fseek(fb, 0, SEEK_SET);\n","\n","    char *hA = (char*)malloc(n);\n","    char *hB = (char*)malloc(m);\n","    if (!hA || !hB) { fprintf(stderr, \"Host alloc fail\\n\"); return 1; }\n","\n","    if (fread(hA, 1, n, fa) != n || fread(hB, 1, m, fb) != m) { fprintf(stderr, \"Read error\\n\"); return 1; }\n","    fclose(fa); fclose(fb);\n","\n","    size_t mat_elems = (size_t)(n + 1) * (m + 1);\n","    size_t mat_bytes = mat_elems * sizeof(int);\n","\n","    char *dA; char *dB; int *dmat;\n","    cuda_check(cudaMalloc((void**)&dA, n), \"cudaMalloc A\");\n","    cuda_check(cudaMalloc((void**)&dB, m), \"cudaMalloc B\");\n","    cuda_check(cudaMalloc((void**)&dmat, mat_bytes), \"cudaMalloc mat\");\n","\n","    cuda_check(cudaMemcpy(dA, hA, n, cudaMemcpyHostToDevice), \"cpy A\");\n","    cuda_check(cudaMemcpy(dB, hB, m, cudaMemcpyHostToDevice), \"cpy B\");\n","    cuda_check(cudaMemset(dmat, 0, mat_bytes), \"memset mat\");\n","\n","    cudaEvent_t start, stop;\n","    cudaEventCreate(&start);\n","    cudaEventCreate(&stop);\n","    cudaEventRecord(start);\n","\n","    int maxk = n + m;\n","    for (int k = 2; k <= maxk; ++k) {\n","        int i_start = (k > m+1) ? (k - (m+1)) : 1;\n","        int i_end = (k - 1 > n) ? n : k - 1;\n","        int len = i_end - i_start + 1;\n","\n","        if (len <= 0) continue;\n","        int threads = blockSize;\n","        int blocks = (len + threads - 1) / threads;\n","        diag_kernel<<<blocks, threads>>>(dmat, dA, dB, n, m, k);\n","    }\n","    cuda_check(cudaGetLastError(), \"kernel launch\");\n","\n","    cudaEventRecord(stop);\n","    cudaEventSynchronize(stop);\n","\n","    float milliseconds = 0;\n","    cudaEventElapsedTime(&milliseconds, start, stop);\n","\n","    int res = 0;\n","    // Fix indice anche qui\n","    size_t res_idx = (size_t)n * (m + 1) + m;\n","    cuda_check(cudaMemcpy(&res, &dmat[res_idx], sizeof(int), cudaMemcpyDeviceToHost), \"cpy result\");\n","\n","    printf(\"RESULT_LEN: %d\\n\", res);\n","    printf(\"ELAPSED_TIME: %.6f\\n\", milliseconds / 1000.0f);\n","\n","    cudaFree(dA); cudaFree(dB); cudaFree(dmat);\n","    free(hA); free(hB);\n","    cudaEventDestroy(start); cudaEventDestroy(stop);\n","    return 0;\n","}\n","\"\"\"\n","\n","# Scriviamo il codice corretto su disco\n","with open(\"lcs_cuda.cu\", \"w\") as f:\n","    f.write(cuda_code_fixed)\n","print(\"Codice lcs_cuda.cu aggiornato con supporto per matrici grandi (size_t).\\n\")\n","\n","\n","# --- PASSO 1: COMPILAZIONE ---\n","print(\"Compilazione in corso...\")\n","!nvcc -O3 lcs_cuda.cu -o lcs_cuda\n","print(\"Compilazione completata.\\n\")\n","\n","\n","# --- PASSO 2: BENCHMARK ---\n","SIZES = [10240, 51200]  # 10KB, 50KB\n","BLOCK_SIZES = [128, 256, 512]\n","REPS = 3\n","\n","print(\"impl,size,rep,param,time_s\")\n","\n","for s in SIZES:\n","    file_a = f\"bench_{s}_A.bin\"\n","    file_b = f\"bench_{s}_B.bin\"\n","\n","    if not os.path.exists(file_a):\n","        !python3 generate_input.py --size-per-file $s --prefix \"bench_{s}\" --seed 42 --alphabet ascii\n","\n","    for r in range(1, REPS + 1):\n","        for b in BLOCK_SIZES:\n","            try:\n","                cmd = f\"./lcs_cuda {file_a} {file_b} {b}\"\n","                output = subprocess.check_output(cmd, shell=True).decode(\"utf-8\")\n","\n","                for line in output.splitlines():\n","                    if \"ELAPSED_TIME:\" in line:\n","                        time_s = line.split(\":\")[1].strip()\n","                        print(f\"cuda,{s},{r},{b},{time_s}\")\n","            except subprocess.CalledProcessError:\n","                print(f\"cuda,{s},{r},{b},ERROR\")"]}]}